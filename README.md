
# Proyecto 4: Detecci√≥n de Expresiones Faciales en Tiempo Real

**Universidad del Valle de Guatemala**  
**Facultad de Ingenier√≠a**  
**Departamento de Ciencias de la Computaci√≥n**  
**Curso: Visi√≥n por Computadora**

## Integrantes

- Diego Leiva ‚Äì 21752  
- Mar√≠a Ram√≠rez ‚Äì 21342  
- Gustavo Gonz√°lez ‚Äì 21438  
- Pablo Orellana ‚Äì 21970  

## Descripci√≥n del Proyecto

Este proyecto consiste en un sistema de reconocimiento de emociones faciales en tiempo real utilizando MediaPipe para detecci√≥n y extracci√≥n de caracter√≠sticas faciales, y un modelo de aprendizaje profundo (MLP) entrenado sobre el dataset FER+ para clasificar emociones b√°sicas.

El flujo principal del sistema es:

1. Captura de video en tiempo real desde la webcam.
2. Detecci√≥n de rostro usando MediaPipe.
3. Extracci√≥n de 3D face landmarks (478 puntos).
4. Clasificaci√≥n de emociones con un modelo preentrenado en PyTorch.
5. Visualizaci√≥n de resultados con bounding box y etiqueta de la emoci√≥n.

## Tecnolog√≠as Utilizadas

- **Python** `3.12.9`
- **PyTorch** para entrenamiento e inferencia del modelo
- **MediaPipe** para detecci√≥n facial y extracci√≥n de landmarks
- **OpenCV** para procesamiento y visualizaci√≥n de video
- **scikit-learn** para m√©tricas de evaluaci√≥n

## Dataset Utilizado

- **FER+** ‚Äì Dataset de expresiones faciales etiquetado con m√∫ltiples anotadores humanos  
  üîó [FER+ en Kaggle](https://www.kaggle.com/datasets/subhaditya/fer2013plus/data)

## Modelos Preentrenados de MediaPipe

- **Detector facial MediaPipe**:  
  üîó [BlazeFace Short-Range](https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector#blazeface_short-range)

- **Face Landmark Model**:  
  üîó [FaceLandmarker](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker#models)

## C√≥mo Ejecutar

1. Instala las dependencias:

    ```bash
    pip install -r requirements.txt
    ```

2. Ejecuta el script principal:

   ```bash
   python src/live_inference.py
   ```

> [!NOTE]
> Aseg√∫rate de tener conectada tu c√°mara web y los modelos `.pth` y `.tflite` descargados en las rutas indicadas.

## Resultados y Evaluaci√≥n

El modelo fue entrenado con FER+ y alcanza resultados aceptables en emociones como felicidad, sorpresa y enojo.
Durante la inferencia en tiempo real, se logra una visualizaci√≥n fluida (\~30 FPS en GPU) con predicciones en vivo y colores diferenciados por emoci√≥n.
